\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsthm,mathtools,amssymb}
\usepackage{amsmath,graphicx,subfigure}
\usepackage{xcolor}
\usepackage{cite}
\usepackage[mathscr]{euscript}
\usepackage{bm}
\usepackage{stmaryrd}
%\usepackage{mdframed}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{boxedminipage}
\usepackage{multicol}

\DeclareMathAlphabet{\mathpzc}{OT1}{pzc}{m}{it}

\newtheorem{theorem}{Theorem}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{property}[theorem]{Property}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{example}[theorem]{Example}

\newcommand{\Perp}{\perp \! \! \! \perp}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\def\Epsilon{E}
\def\Eta{H}

\def\veczero{{\mathbf 0}}
\def\vec1{{\mathbf 1}}
\def\matzero{{\mathbf 0}}

\def\veca{{\mathbf a}}
\def\vecb{{\mathbf b}}
\def\vecc{{\mathbf c}}
\def\vecd{{\mathbf d}}
\def\vece{{\mathbf e}}
\def\vecf{{\mathbf f}}
\def\vecg{{\mathbf g}}
\def\vech{{\mathbf h}}
\def\veck{{\mathbf k}}
\def\vecl{{\mathbf l}}
\def\vecm{{\mathbf m}}
\def\vecn{{\mathbf n}}
\def\vecp{{\mathbf p}}
\def\vecq{{\mathbf q}}
\def\vecr{{\mathbf r}}
\def\vect{{\mathbf t}}
\def\vecu{{\mathbf u}}
\def\vecv{{\mathbf v}}
\def\vecw{{\mathbf w}}
\def\vecx{{\mathbf x}}
\def\vecy{{\mathbf y}}
\def\vecz{{\mathbf z}}

\def\vecX{{\mathbf X}}
\def\vecY{{\mathbf Y}}
\def\vecZ{{\mathbf Z}}

\def\vecalpha{{\mathbf \alpha}}
\def\vecbeta{{\mathbf \beta}}
\def\veceta{{\mathbf \eta}}
\def\vecepsilon{{\boldsymbol \epsilon}}
\def\veclambda{{\mathbf \lambda}}
\def\vecvarepsilon{{\boldsymbol \varepsilon}}
\def\vecgamma{{\boldsymbol \gamma}}
\def\vecmu{{\boldsymbol \mu}}
\def\vecnu{{\boldsymbol \nu}}
\def\vecomega{{\boldsymbol \omega}}
\def\vecphi{{\boldsymbol \phi}}
\def\vecpsi{{\boldsymbol \psi}}
\def\vecsigma{{\mathbf \sigma}}
\def\vectau{{\boldsymbol \tau}}
\def\vecupsilon{{\boldsymbol \upsilon}}
\def\vecvarphi{{\boldsymbol \varphi}}
\def\vecxi{{\boldsymbol \xi}}
\def\veczeta{{\boldsymbol \zeta}}

\def\vecU{{\mathbf U}}
\def\vecV{{\mathbf V}}
\def\vecX{{\mathbf X}}
\def\vecY{{\mathbf Y}}
\def\vecZ{{\mathbf Z}}

\def\vecEpsilon{{\mathbf \Epsilon}}
\def\vecEta{{\mathbf \Eta}}


\def\matA{{\mathbf A}}
\def\matB{{\mathbf B}}
\def\matC{{\mathbf C}}
\def\matD{{\mathbf D}}
\def\matE{{\mathbf E}}
\def\matF{{\mathbf F}}
\def\matH{{\mathbf H}}
\def\matI{{\mathbf I}}
\def\matK{{\mathbf K}}
\def\matL{{\mathbf L}}
\def\matOmega{{\mathbf \Omega}}
\def\matO{{\mathbf O}}
\def\matN{{\mathbf N}}
\def\matP{{\mathbf P}}
\def\matS{{\mathbf S}}
\def\matU{{\mathbf U}}
\def\matV{{\mathbf V}}
\def\matW{{\mathbf W}}
\def\matX{{\mathbf X}}
\def\matZ{{\mathbf Z}}
\def\matGamma{{\mathbf{\mathrm{\Gamma}}}}
\def\matLambda{{\mathbf{\mathrm{\Lambda}}}}
\def\matOmega{{\mathbf{\mathrm{\Omega}}}}
\def\matPhi{{\mathbf \Phi}}
\def\matPsi{{\mathbf \Psi}}
\def\matRho{{\mathbf{\mathrm{P}}}}
\def\matSigma{{\mathbf{\mathrm{\Sigma}}}}
\def\matUpsilon{{\mathbf{\mathrm{\Upsilon}}}}
\def\matXi{{\mathbf{\mathrm{\Xi}}}}

\def\matzero{{\mathbf 0}}


\def\complex{{\mathbb {C}}}
\def\real{{\mathbb {R}}}
\def\rational{{\mathbb {Q}}}
\def\pnint{{\mathbb {Z}}}
\def\nnint{{\mathbb{Z}_+}}
\def\pint{{\mathbb {N}}}
\def\nnreal{{\real_{\geq 0}}}

\def\defas{\overset{\mbox{def}}{=}}
\def\as{\overset{\mbox{a.s.}}{=}}
\def\ind{1}
\def\normal{\calN}
\def\expect{\mathbb{E}}
\def\variance{\mbox{Var}}
\def\prob{\mathbb{P}}
\def\risk{\calR}
\def\Gaussian{{\cal{N}}}
\def\Uniform{{\cal{U}}}
\def\Trace{\mbox{Tr}}
\def\sign{\mbox{sign}}
\def\evaloss{\star}
\def\margin{\varrho}
\def\det{\mbox{det}}

\def\calA{{\cal A}}
\def\calE{{\cal E}}
\def\calF{{\cal F}}
\def\calH{{\cal H}}
\def\calI{{\cal I}}
\def\calL{{\cal L}}
\def\calN{{\cal N}}
\def\calR{{\cal R}}
\def\calS{{\cal S}}
\def\calX{{\cal X}}
\def\calY{{\cal Y}}
\def\calZ{{\cal Z}}

\def\scrA{\mathscr{A}}
\def\scrB{\mathscr{B}}
\def\scrC{\mathscr{C}}
\def\scrD{\mathscr{D}}
\def\scrE{\mathscr{E}}
\def\scrF{\mathscr{F}}
\def\scrH{\mathscr{H}}
\def\scrI{\mathscr{I}}
\def\scrJ{\mathscr{J}}
\def\scrL{\mathscr{L}}
\def\scrM{\mathscr{M}}
\def\scrN{\mathscr{N}}
\def\scrP{\mathscr{P}}
\def\scrR{\mathscr{R}}
\def\scrS{\mathscr{S}}
\def\scrX{\mathscr{X}}
\def\pzcy{\mathpzc{y}}

\def\frakM{\mathfrak{M}}

\renewcommand\floatpagefraction{.9}
\renewcommand\topfraction{.9}
\renewcommand\bottomfraction{.9}
\renewcommand\textfraction{.1}
\renewcommand{\baselinestretch}{1}

\def\quesColor{blue}
\def\solColor{black}


\title{HW2 Handwritten Assignment}
\author{Lecturor: Pei-Yuan Wu\\
TAs: {Yuan-Chia Chang(Problem 5), Chun-Lin Huang(Problem 1, 2, 3, 4)}}
\date{October 2023, First Edition}
\begin{document}

\maketitle

\section*{Problem 1 (Convolution)(0.5\%)}
As mentioned in class, image size may change after convolution layers. Consider a batch of image data with shape $(B, W, H, input\_channels)$, how will the shape change after the following convolution layer:
\begin{footnotesize}
\begin{equation*}
Conv2D \left( input\_channels, output\_channels, kernel\_size=(k_1,k_2), stride=(s_1,s_2), padding=(p_1,p_2) \right)
\end{equation*}
\end{footnotesize}
%
For simplicity, the padding tuple means that $p_1$ pixels are padded on both left and right sides, and $p_2$ pixels are padded on both top and bottom sides.

\section*{Problem 2 (Batch Normalization)(1\%)}
{\bf Batch normalization}\cite{Ioffe2015} is a popular trick for training deep networks nowadays, which aims to preserve the distribution within hidden layers and avoids vanishing gradient issue.  The alogrithm can be written as below:
\begin{algorithm}
  \caption{Batch Normalization}
  \label{alg:batch_normalization}
  \hspace*{\algorithmicindent} \textbf{Input} Feature from data points over a mini-batch $B = (x_i)_{i=1}^m$\\
  \hspace*{\algorithmicindent} \textbf{Output} $y_i = {BN}_{\gamma,\beta}(x_i)$
  \begin{algorithmic}[1]
    \Procedure{BatchNormalize}{$B$, $\gamma$, $\beta$}
    \State $\mu_B \gets \frac{1}{m}\sum_{i=1}^m x_i$ 					\Comment{mini-batch mean}
    \State $\sigma^2_B \gets \frac{1}{m}\sum_{i=1}^m (x_i-\mu_B)^2$ 		\Comment{mini-batch variance}
    \For{$i \gets 1$ to $m$}
      \State ${\hat x}_i \gets \frac{x_i-\mu_B}{\sqrt{\sigma_{B}^{2}+\epsilon}}$ 	\Comment{normalize}
      \State $y_i \gets \gamma {\hat x}_i + \beta$ 							\Comment{scale and shift}
    \EndFor
    \State \Return 
    \EndProcedure
   \end{algorithmic}
\end{algorithm}
%

During training we need to backpropagate the gradient of loss $\ell$ through this transformation, as well as compute the gradients with respect to the parameters $\gamma$, $\beta$.  Towards this end, please write down the close form expressions for $\frac{\partial \ell}{\partial x_i}$, $\frac{\partial \ell}{\partial \gamma}$, $\frac{\partial \ell}{\partial \beta}$ in terms of $x_i$, $\mu_B$, $\sigma_B^2$, ${\hat x}_i$, $y_i$ (given by the forward pass) and $\frac{\partial \ell}{\partial y_i}$ (given by the backward pass).

- Hint: You may first write down the close form expressions of $\frac{\partial \ell}{\partial {\hat x}_i}$, $\frac{\partial \ell}{\partial \sigma_B^2}$, $\frac{\partial \ell}{\partial \mu_B}$, and then use them to compute $\frac{\partial \ell}{\partial x_i}$, $\frac{\partial \ell}{\partial \gamma}$, $\frac{\partial \ell}{\partial \beta}$.



\section*{Problem 3 (Constrained Mahalanobis Distance Minimization Problem)(1.5\%)}
\begin{enumerate}
    \item Let $\Sigma \in R^{m \times m}$ be a symmetric positive semi-definite matrix, $\mu \in R^m$.  Please construct a set of points $x_1,...,x_n \in R^m$ such that
$$\frac{1}{n}\sum_{i=1}^n (x_i - \mu) (x_i - \mu)^T = \Sigma, ~~ \frac{1}{n}\sum_{i=1}^n x_i = \mu$$

- Find the relation between set of points and ($\mu$, $\Sigma$) and $(\mu, \Sigma)$ is known
\item Let $1 \leq k \leq m$, solve the following optimization problem (and justify with proof): \\
minimize  $\quad Trace(\Phi^T \Sigma \Phi)$ \\ 
subject to $\quad \Phi^T \Phi = I_k$ \\
variables $\quad \Phi \in R^{m \times k}$
\end{enumerate}

\begin{thebibliography}{50}
	\bibitem{Ioffe2015} Sergey Ioffe and Christian Szegedy (2015), ``Batch Normalization: Accelerating Deep Network Training b
y Reducing Internal Covariate Shift", Arxiv:1502.03167
\end{thebibliography}

\section*{Problem 4 (Convergence of K-means Clustering)
(1.5\%)}
 In the K-means clustering algorithm, we are given a set of $n$ points $x_i \in \mathbb{R}^d, i \in\{1, \ldots, n\}$ and we want to find the centers of $k$ clusters $\boldsymbol{\mu}=\left(\mu_1, \ldots, \mu_k\right)$ by minimizing the average distance from the points to the closest cluster center. In general, $n\geq k$. Define function $\mathcal{C} :\{1, \cdots, n\}\rightarrow \{1, 2,\cdots, k\}$ assigns one of $k$ clusters to each point in the data set such that $\mathcal{C}(i) = q$ if the $i$-th data point $x_i$ is assigned to the $q$-th cluster where $i\in\{1, 2, \cdots, n\}$ and $q\in\{1, 2, \cdots, k\}$

Formally, we want to minimize the following loss function

$$
L(\mathcal{C}, \boldsymbol{\mu})=\sum_{i=1}^n\left\|x_i-\mu_{\mathcal{C}(i)}\right\|_2^2 = \sum_{q=1}^{k}\sum_{i:\mathcal{C}(i)=q}\left\|x_i-\mu_{q}\right\|_2^2
$$

The K-means algorithm:
\begin{algorithm}
\caption{K-means algorithm}

Initialize cluster center $\mu_j, j = 1, 2, \cdots, k$ ($k$ random $x_n$ from data set)

Repeat:
\begin{enumerate}
    \item Fix $\boldsymbol{\mu}$, update $\mathcal{C}(i)$ for each $i$ that minimizes $L$. Formally, consider a data point $x_i$, and let $\mathcal{C}(i)$ be the  assignment from the previous iteration and $C^*(i)$ be the new assignment obtained as: $C^*(i)= \arg\min_{j=1, \cdots, k} \Vert x_i - \mu_j\Vert^2_2$
    \item Fix $\mathcal{C}$, update the centers $\mu_j$ which satisfies 
$$\left|\left\{i: \mathcal{C}(i)=j\right\}\right|\mu_j= 
\sum_{i: \mathcal{C}(i)=j} x_i, 
$$for each $j$, where $\left|\left\{i: \mathcal{C}(i)=j\right\}\right|$ is the number of elements of set $\left\{i: \mathcal{C}(i)=j\right\}$.(i.e. Set the cluster centres to be the means of the points in each cluster.)
\end{enumerate}
\end{algorithm}

The algorithm stops when no change in loss function occurs during the assignment step.

Suppose that the algorithm proceeds from iteration $t$ to $t+1$.
\begin{enumerate}
    \item Consider the points $z_1, z_2, \cdots, z_m$, where $m\geq 1$ . and for $i\in\{1, 2, \cdots, m\}, z_i\in \mathbb{R}^d$. Let $\bar{z} = \frac{1}{m}\sum_{i=1}^m z_i$ be the mean of these points, and let $z\in \mathbb{R}^d$ be an arbitrary point in the same ($d$-dimensional) space. Then
    $$
    \sum_{i=1}^m \Vert z_i - z\Vert^2_2 \geq \sum_{i=1}^m \Vert z_i - \bar{z}\Vert^2_2
    $$
    \item Show that $L(\mathcal{C}^{t+1}, \boldsymbol{\mu}^{t}) \leq L(\mathcal{C}^{t}, \boldsymbol{\mu}^{t})$ i.e. The first step in K-means clustering

    \item Show that $L(\mathcal{C}^{t+1}, \boldsymbol{\mu}^{t+1})\leq L(\mathcal{C}^{t+1}, \boldsymbol{\mu}^{t})$ i.e. The second step in K-means clustering. (Hint: Use the result of (a))

    \item Use the result in (b) and $(c)$ to show that the loss of $k$-means clustering algorithm is monotonic decreasing.(Hint: Show that the sequence $\{l_t\}$, where $l_t = L(\mathcal{C}^{t}, \boldsymbol{\mu}^{t})$, which is monotone decreasing ($l_{t+1}\leq l_t, \forall t$) and bounded below ($l_t\geq 0$). Then, we use monotone convergence theorem for sequences, $\{l_t\}$ converges.)

    \item Show that the $k$-means clustering algorithm converges in finitely many steps.
\end{enumerate}

\section*{Problem 5 (Gradient Descent Convergence) (1.5\%)}
 Suppose the function $f:\mathbb{R}^n \rightarrow \mathbb{R}$ is differentiable. Also, $f$ is $\beta$-smoothness and $\alpha$-strongly convex. \\
 $$\beta-smoothness: \beta > 0, \forall \boldsymbol{x}, \boldsymbol{y} \in \mathbb{R}^n, 
 \lVert \nabla f(\boldsymbol{x})- \nabla f(\boldsymbol{y}) \rVert_2  \leq  \beta \lVert \boldsymbol{x} - \boldsymbol{y} \rVert_2$$ \\
 $$\alpha-strongly \ convex: \alpha > 0, \forall \boldsymbol{x}, \boldsymbol{y} \in \mathbb{R}^n, 
 f(\boldsymbol{x})-f(\boldsymbol{y})-\nabla f(\boldsymbol{y})^T (\boldsymbol{x} - \boldsymbol{y}) \geq \frac{\alpha}{2} \lVert \boldsymbol{x} - \boldsymbol{y} \rVert_2^2$$ \\
 Then we propose a gradient descent algorithm
 \begin{enumerate}
    \item Find a initial $\boldsymbol{\theta}^0.$
     \item Let $\boldsymbol{\theta}^{n+1} = \boldsymbol{\theta}^{n} - \eta \nabla_{\boldsymbol{\theta}}f(\boldsymbol{\theta}^n) \ \forall n \geq 0$, where $\eta=\frac{1}{\beta}.$
 \end{enumerate}
The following problems lead you to prove the gradient descent convergence.
\begin{enumerate}
\item Prove the property of $\beta$-smoothness function
 $$\forall \boldsymbol{x}, \boldsymbol{y} \in \mathbb{R}^n, 
 f(\boldsymbol{x})-f(\boldsymbol{y})-\nabla f(\boldsymbol{y})^T (\boldsymbol{x}-\boldsymbol{y}) \leq \frac{\beta}{2} \lVert \boldsymbol{x}-\boldsymbol{y} \rVert_2^2$$
 \begin{enumerate}
 \item Define $g: \mathbb{R} \rightarrow \mathbb{R}, g(t)=f(\boldsymbol{y}+t(\boldsymbol{x}-\boldsymbol{y}))$. Show that $f(\boldsymbol{x})-f(\boldsymbol{y})=\int_{0}^{1} g^{'}(t) \,dt$.
 \item Show that $g^{'}(t)= \nabla f(\boldsymbol{y}+t(\boldsymbol{x}-\boldsymbol{y}))^T(\boldsymbol{x}-\boldsymbol{y})$.
 \item Show that $|f(\boldsymbol{x})-f(\boldsymbol{y})- \nabla f(\boldsymbol{y})^T(\boldsymbol{x}-\boldsymbol{y})| \leq \int_{0}^{1} |(\nabla f(\boldsymbol{y}+t(\boldsymbol{x}-\boldsymbol{y})) - \nabla f(\boldsymbol{y}))^T(\boldsymbol{x}-\boldsymbol{y})| \, dt$.
 \item By Cauchy-Schwarz inequality and the definition of $\beta$-smoothness, show that $|f(\boldsymbol{x})-f(\boldsymbol{y})- \nabla f(\boldsymbol{y})^T(\boldsymbol{x}-\boldsymbol{y})| \leq \frac{\beta}{2} \lVert \boldsymbol{x}-\boldsymbol{y} \rVert_2^2$, hence we get $$f(\boldsymbol{x})-f(\boldsymbol{y})-\nabla f(\boldsymbol{y})^T (\boldsymbol{x}-\boldsymbol{y}) \leq \frac{\beta}{2} \lVert \boldsymbol{x}-\boldsymbol{y} \rVert_2^2$$
 \end{enumerate}
 \item Let $\boldsymbol{y} = \boldsymbol{x} - \frac{1}{\beta} \nabla f(\boldsymbol{x})$ and use 1., prove that 
 $$f(\boldsymbol{x} - \frac{1}{\beta} \nabla f(\boldsymbol{x})) - f(\boldsymbol{x}) \leq -\frac{1}{2\beta} \lVert \nabla f(\boldsymbol{x}) \rVert_2^2$$
 and 
 $$f(\boldsymbol{x}^{*}) - f(\boldsymbol{x}) \leq -\frac{1}{2\beta} \lVert \nabla f(\boldsymbol{x}) \rVert_2^2,$$
 where $\boldsymbol{x}^{*} = \mathop{\arg\min}\limits_{\boldsymbol{x}} f(\boldsymbol{x})$.
 \item Show that $\forall n \geq 0$, 
 $$\lVert \boldsymbol{\theta}^{n+1} - \boldsymbol{\theta}^{*} \rVert_2^2= \lVert \boldsymbol{\theta}^{n} - \boldsymbol{\theta}^{*} \rVert_2^2 + \eta^2\lVert \nabla_{\boldsymbol{\theta}} f(\boldsymbol{\theta}^n) \rVert_2^2 -2 \eta \nabla_{\boldsymbol{\theta}} f(\boldsymbol{\theta}^n)^T(\boldsymbol{\theta}^n-\boldsymbol{\theta}^{*}),$$ where $\boldsymbol{\theta}^{*} = \mathop{\arg\min}\limits_{\boldsymbol{\theta}} f(\boldsymbol{\theta})$.\\
 \item Use 2. and the definition of $\alpha$-strongly convex to prove $\forall n \geq 0$
 $$ \lVert \boldsymbol{\theta}^{n+1} - \boldsymbol{\theta}^{*} \rVert_2^2 \leq (1- \frac{\alpha}{\beta}) \lVert \boldsymbol{\theta}^{n} - \boldsymbol{\theta}^{*} \rVert_2^2, $$ where $\boldsymbol{\theta}^{*} = \mathop{\arg\min}\limits_{\boldsymbol{\theta}} f(\boldsymbol{\theta})$.\\
 \item Use the above inequality to show that $\boldsymbol{\theta}^n$ will converge to $\boldsymbol{\theta}^*$ when $n$ goes to infinity.
 \end{enumerate}

 \section*{Version Description}
 \begin{enumerate}
     \item First Edition: Finish Problem 1 to 5
 \end{enumerate}
\end{document}
