\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsthm,mathtools,amssymb}
\usepackage{amsmath,graphicx,subfigure}
\usepackage{xcolor}
\usepackage{cite}
\usepackage[mathscr]{euscript}
\usepackage{bm}
\usepackage{stmaryrd}
%\usepackage{mdframed}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{boxedminipage}
\usepackage{multicol}

\DeclareMathAlphabet{\mathpzc}{OT1}{pzc}{m}{it}

\newtheorem{theorem}{Theorem}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{property}[theorem]{Property}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{example}[theorem]{Example}

\newcommand{\Perp}{\perp \! \! \! \perp}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\def\Epsilon{E}
\def\Eta{H}

\def\veczero{{\mathbf 0}}
\def\vec1{{\mathbf 1}}
\def\matzero{{\mathbf 0}}

\def\veca{{\mathbf a}}
\def\vecb{{\mathbf b}}
\def\vecc{{\mathbf c}}
\def\vecd{{\mathbf d}}
\def\vece{{\mathbf e}}
\def\vecf{{\mathbf f}}
\def\vecg{{\mathbf g}}
\def\vech{{\mathbf h}}
\def\veck{{\mathbf k}}
\def\vecl{{\mathbf l}}
\def\vecm{{\mathbf m}}
\def\vecn{{\mathbf n}}
\def\vecp{{\mathbf p}}
\def\vecq{{\mathbf q}}
\def\vecr{{\mathbf r}}
\def\vect{{\mathbf t}}
\def\vecu{{\mathbf u}}
\def\vecv{{\mathbf v}}
\def\vecw{{\mathbf w}}
\def\vecx{{\mathbf x}}
\def\vecy{{\mathbf y}}
\def\vecz{{\mathbf z}}

\def\vecX{{\mathbf X}}
\def\vecY{{\mathbf Y}}
\def\vecZ{{\mathbf Z}}

\def\vecalpha{{\mathbf \alpha}}
\def\vecbeta{{\mathbf \beta}}
\def\veceta{{\mathbf \eta}}
\def\vecepsilon{{\boldsymbol \epsilon}}
\def\veclambda{{\mathbf \lambda}}
\def\vecvarepsilon{{\boldsymbol \varepsilon}}
\def\vecgamma{{\boldsymbol \gamma}}
\def\vecmu{{\boldsymbol \mu}}
\def\vecnu{{\boldsymbol \nu}}
\def\vecomega{{\boldsymbol \omega}}
\def\vecphi{{\boldsymbol \phi}}
\def\vecpsi{{\boldsymbol \psi}}
\def\vecsigma{{\mathbf \sigma}}
\def\vectau{{\boldsymbol \tau}}
\def\vecupsilon{{\boldsymbol \upsilon}}
\def\vecvarphi{{\boldsymbol \varphi}}
\def\vecxi{{\boldsymbol \xi}}
\def\veczeta{{\boldsymbol \zeta}}

\def\vecU{{\mathbf U}}
\def\vecV{{\mathbf V}}
\def\vecX{{\mathbf X}}
\def\vecY{{\mathbf Y}}
\def\vecZ{{\mathbf Z}}

\def\vecEpsilon{{\mathbf \Epsilon}}
\def\vecEta{{\mathbf \Eta}}


\def\matA{{\mathbf A}}
\def\matB{{\mathbf B}}
\def\matC{{\mathbf C}}
\def\matD{{\mathbf D}}
\def\matE{{\mathbf E}}
\def\matF{{\mathbf F}}
\def\matH{{\mathbf H}}
\def\matI{{\mathbf I}}
\def\matK{{\mathbf K}}
\def\matL{{\mathbf L}}
\def\matOmega{{\mathbf \Omega}}
\def\matO{{\mathbf O}}
\def\matN{{\mathbf N}}
\def\matP{{\mathbf P}}
\def\matS{{\mathbf S}}
\def\matU{{\mathbf U}}
\def\matV{{\mathbf V}}
\def\matW{{\mathbf W}}
\def\matX{{\mathbf X}}
\def\matZ{{\mathbf Z}}
\def\matGamma{{\mathbf{\mathrm{\Gamma}}}}
\def\matLambda{{\mathbf{\mathrm{\Lambda}}}}
\def\matOmega{{\mathbf{\mathrm{\Omega}}}}
\def\matPhi{{\mathbf \Phi}}
\def\matPsi{{\mathbf \Psi}}
\def\matRho{{\mathbf{\mathrm{P}}}}
\def\matSigma{{\mathbf{\mathrm{\Sigma}}}}
\def\matUpsilon{{\mathbf{\mathrm{\Upsilon}}}}
\def\matXi{{\mathbf{\mathrm{\Xi}}}}

\def\matzero{{\mathbf 0}}


\def\complex{{\mathbb {C}}}
\def\real{{\mathbb {R}}}
\def\rational{{\mathbb {Q}}}
\def\pnint{{\mathbb {Z}}}
\def\nnint{{\mathbb{Z}_+}}
\def\pint{{\mathbb {N}}}
\def\nnreal{{\real_{\geq 0}}}

\def\defas{\overset{\mbox{def}}{=}}
\def\as{\overset{\mbox{a.s.}}{=}}
\def\ind{1}
\def\normal{\calN}
\def\expect{\mathbb{E}}
\def\variance{\mbox{Var}}
\def\prob{\mathbb{P}}
\def\risk{\calR}
\def\Gaussian{{\cal{N}}}
\def\Uniform{{\cal{U}}}
\def\Trace{\mbox{Tr}}
\def\sign{\mbox{sign}}
\def\evaloss{\star}
\def\margin{\varrho}
\def\det{\mbox{det}}

\def\calA{{\cal A}}
\def\calE{{\cal E}}
\def\calF{{\cal F}}
\def\calH{{\cal H}}
\def\calI{{\cal I}}
\def\calL{{\cal L}}
\def\calN{{\cal N}}
\def\calR{{\cal R}}
\def\calS{{\cal S}}
\def\calX{{\cal X}}
\def\calY{{\cal Y}}
\def\calZ{{\cal Z}}

\def\scrA{\mathscr{A}}
\def\scrB{\mathscr{B}}
\def\scrC{\mathscr{C}}
\def\scrD{\mathscr{D}}
\def\scrE{\mathscr{E}}
\def\scrF{\mathscr{F}}
\def\scrH{\mathscr{H}}
\def\scrI{\mathscr{I}}
\def\scrJ{\mathscr{J}}
\def\scrL{\mathscr{L}}
\def\scrM{\mathscr{M}}
\def\scrN{\mathscr{N}}
\def\scrP{\mathscr{P}}
\def\scrR{\mathscr{R}}
\def\scrS{\mathscr{S}}
\def\scrX{\mathscr{X}}
\def\pzcy{\mathpzc{y}}

\def\frakM{\mathfrak{M}}

\renewcommand\floatpagefraction{.9}
\renewcommand\topfraction{.9}
\renewcommand\bottomfraction{.9}
\renewcommand\textfraction{.1}
\renewcommand{\baselinestretch}{1}

\def\solColor{blue}


\title{HW2 Handwritten Assignment Solution}
\author{Lecturor: Pei-Yuan Wu\\
TAs: {Yuan-Chia Chang(Problem 5), Chun-Lin Huang(Problem 1, 2, 3, 4)}}
\date{November 2023, First Edition}
\begin{document}

\maketitle

\section*{Problem 1 (Convolution)(0.5\%)}
As mentioned in class, image size may change after convolution layers. Consider a batch of image data with shape $(B, W, H, input\_channels)$, how will the shape change after the following convolution layer:
\begin{footnotesize}
\begin{equation*}
Conv2D \left( input\_channels, output\_channels, kernel\_size=(k_1,k_2), stride=(s_1,s_2), padding=(p_1,p_2) \right)
\end{equation*}
\end{footnotesize}
%
For simplicity, the padding tuple means that $p_1$ pixels are padded on both left and right sides, and $p_2$ pixels are padded on both top and bottom sides.
\textcolor{\solColor}{\textit{Solution:}
The output is $(B, W^\prime, H^\prime, output\_channels)$, where
$$
\begin{aligned}
&W^{\prime}=\left\lfloor\frac{W+2 * p_1-k_1}{s_1}+1\right\rfloor \\
&H^{\prime}=\left\lfloor\frac{H+2 * p_2-k_2}{s_2}+1\right\rfloor
\end{aligned}
$$
Note that you should give some explanation to get all points.}

\newpage
\section*{Problem 2 (Batch Normalization)(1\%)}
{\bf Batch normalization}\cite{Ioffe2015} is a popular trick for training deep networks nowadays, which aims to preserve the distribution within hidden layers and avoids vanishing gradient issue.  The alogrithm can be written as below:
\begin{algorithm}
  \caption{Batch Normalization}
  \label{alg:batch_normalization}
  \hspace*{\algorithmicindent} \textbf{Input} Feature from data points over a mini-batch $B = (x_i)_{i=1}^m$\\
  \hspace*{\algorithmicindent} \textbf{Output} $y_i = {BN}_{\gamma,\beta}(x_i)$
  \begin{algorithmic}[1]
    \Procedure{BatchNormalize}{$B$, $\gamma$, $\beta$}
    \State $\mu_B \gets \frac{1}{m}\sum_{i=1}^m x_i$ 					\Comment{mini-batch mean}
    \State $\sigma^2_B \gets \frac{1}{m}\sum_{i=1}^m (x_i-\mu_B)^2$ 		\Comment{mini-batch variance}
    \For{$i \gets 1$ to $m$}
      \State ${\hat x}_i \gets \frac{x_i-\mu_B}{\sqrt{\sigma_{B}^{2}+\epsilon}}$ 	\Comment{normalize}
      \State $y_i \gets \gamma {\hat x}_i + \beta$ 							\Comment{scale and shift}
    \EndFor
    \State \Return 
    \EndProcedure
   \end{algorithmic}
\end{algorithm}
%

During training we need to backpropagate the gradient of loss $\ell$ through this transformation, as well as compute the gradients with respect to the parameters $\gamma$, $\beta$.  Towards this end, please write down the close form expressions for $\frac{\partial \ell}{\partial x_i}$, $\frac{\partial \ell}{\partial \gamma}$, $\frac{\partial \ell}{\partial \beta}$ in terms of $x_i$, $\mu_B$, $\sigma_B^2$, ${\hat x}_i$, $y_i$ (given by the forward pass) and $\frac{\partial \ell}{\partial y_i}$ (given by the backward pass).

- Hint: You may first write down the close form expressions of $\frac{\partial \ell}{\partial {\hat x}_i}$, $\frac{\partial \ell}{\partial \sigma_B^2}$, $\frac{\partial \ell}{\partial \mu_B}$, and then use them to compute $\frac{\partial \ell}{\partial x_i}$, $\frac{\partial \ell}{\partial \gamma}$, $\frac{\partial \ell}{\partial \beta}$.

\textcolor{\solColor}{\textit{Solution:}
We use the gradient descent to update $\gamma$ and $\beta$:
\begin{align*}
    &\gamma \leftarrow \gamma - \eta\frac{\partial l}{\partial \gamma} \\ 
    &\beta\leftarrow \beta - \eta\frac{\partial l}{\partial \beta}
\end{align*}
where $\eta$ is a learning rate and 
\begin{align*}
    &\frac{\partial l}{\partial \gamma} = \sum_{i=1}^m\left(\frac{\partial l}{\partial y_i}\frac{\partial y_i}{\partial \gamma}\right) = \sum_{i=1}^m\frac{\partial l}{\partial y_i}\hat{x_i}\\
    &\frac{\partial l}{\partial \beta} = \sum_{i=1}^m\left(\frac{\partial l}{\partial y_i}\frac{\partial y_i}{\partial \beta}\right) = \sum_{i=1}^m\frac{\partial l}{\partial y_i}
\end{align*}
Note that we sum from $1$ to $m$ because we are working with mini-batches.
Now, we derive some important terms by chain rule:
\begin{align*}
    &\frac{\partial l}{\partial \hat{x_i}} = \frac{\partial l}{\partial y_i}\frac{\partial y_i}{\partial \hat{x_i}} = \frac{\partial l}{\partial y_i}\gamma\\
    &\frac{\partial l}{\partial \sigma^2_B} = \frac{\partial l}{\partial \hat{x_i}}\frac{\partial \hat{x_i}}{\partial \sigma^2_B} = -\frac{1}{2}\sum_{i=1}^m\frac{\partial l}{\partial \hat{x_i}}(x_i-\mu_B)(\sigma_B^2+\epsilon)^{-\frac{3}{2}}
\end{align*}
\begin{align*}
    \frac{\partial l}{\partial \mu_B}  &= \frac{\partial l}{\partial \hat{x_i}}\frac{\partial \hat{x_i}}{\partial \mu_B}\\
    &= \sum_{i=1}^m \frac{\partial l}{\partial \hat{x_i}} \frac{\partial}{\partial \mu_B} (x_i-\mu_B)(\sigma^2_B+\epsilon)^{-\frac{1}{2}}\\
    &= \sum_{i=1}^{m}\frac{\partial l}{\partial \hat{x_i}}\left[ \frac{\partial (x_i - \mu_B)}{\partial \mu_B}(\sigma^2_B + \epsilon)^{-\frac{1}{2}} + (x_i - \mu_B)\frac{\partial (\sigma^2_B + \epsilon)^{-\frac{1}{2}}}{\partial \mu_B} \right]\\
    &= \sum_{i=1}^{m}\frac{\partial l}{\partial \hat{x_i}}\left[\frac{-1}{\sqrt{\sigma^2_B + \epsilon}} + (x_i - \mu_B)\frac{\partial (\sigma^2_B + \epsilon)^{-\frac{1}{2}}}{\partial \mu_B} \right]
\end{align*}
We know $\sigma^2_B = \frac{1}{m} \sum^{m}_{i=1}(x_i-\mu _B)^2$, so the second term in bracket is
\begin{align*}
    (x_i - \mu_B)\frac{\partial (\sigma^2_B + \epsilon)^{-\frac{1}{2}}}{\partial \mu_B} &= (x_i-\mu_B)\frac{-1}{2}(\sigma^2_B + \epsilon)^{-\frac{3}{2}}\frac{\partial (\sigma^2_B + \epsilon)}{\partial\mu_B}\\
    &=\frac{-1}{2}(x_i - \mu_B)(\sigma^2_B + \epsilon)^{-\frac{3}{2}}\frac{\partial\left(\frac{1}{m} \sum^{m}_{i=1}(x_i-\mu _B)^2 + \epsilon\right)}{\partial\mu_B}\\
    &= \frac{-1}{2}(x_i - \mu_B)(\sigma^2_B + \epsilon)^{-\frac{3}{2}}\left(\frac{-2}{m}\sum_{i=1}^m(x_i-\mu_B)\right)
\end{align*}
Hence, 
\begin{align*}
    \frac{\partial l}{\partial \mu_B}  &= \sum_{i=1}^{m}\frac{\partial l}{\partial \hat{x_i}}\frac{-1}{\sqrt{\sigma^2_B + \epsilon}} + \underbrace{\sum_{i=1}^{m}\frac{\partial l}{\partial \hat{x_i}}\frac{-1}{2}(x_i - \mu_B)(\sigma^2_B + \epsilon)^{-\frac{3}{2}}}_{\frac{\partial l}{\partial \sigma^2_B}}\left(\frac{-2}{m}\sum_{i=1}^m(x_i-\mu_B)\right)\\
    &= \sum_{i=1}^{m}\frac{\partial l}{\partial \hat{x_i}}\frac{-1}{\sqrt{\sigma^2_B + \epsilon}} + \frac{\partial l}{\partial \sigma^2_B}\left(\frac{-2}{m}\sum_{i=1}^m(x_i-\mu_B)\right)
\end{align*}
To derive $\frac{\partial l}{\partial x_i}$, we use the chain rule $\frac{\partial l}{\partial x_i} = \frac{\partial l}{\partial \hat{x_i}}\frac{\partial \hat{x_i}}{\partial x_i} + \frac{\partial l}{\partial \sigma^2_B}\frac{\partial \sigma^2_B}{\partial x_i} + \frac{\partial l}{\partial \mu_B}\frac{\partial \mu_B}{\partial x_i}$. Now, calculate the remaining term:
\begin{align*}
    &\frac{\partial \hat{x_i}}{\partial x_i} = \frac{1}{\sqrt{\sigma^2_B + \epsilon}}\\
    &\frac{\partial \mu_B}{\partial x_i} = \frac{1}{m}\\
    &\frac{\partial \sigma^2_B}{\partial x_i} = \frac{2(x_i - \mu)}{m}
\end{align*}
That is, 
\begin{align*}
    \frac{\partial l}{\partial x_i} &= \frac{\partial l}{\partial \hat{x_i}}\frac{1}{\sqrt{\sigma^2_B + \epsilon}} + \frac{\partial l}{\partial \sigma^2_B}\frac{2(x_i - \mu)}{m} + \frac{1}{m}\frac{\partial l}{\partial \mu_B}
\end{align*}}


\newpage
\section*{Problem 3 (Constrained Mahalanobis Distance Minimization Problem)(1.5\%)}
\begin{enumerate}
    \item Let $\Sigma \in R^{m \times m}$ be a symmetric positive semi-definite matrix, $\mu \in R^m$.  Please construct a set of points $x_1,...,x_n \in R^m$ such that
$$\frac{1}{n}\sum_{i=1}^n (x_i - \mu) (x_i - \mu)^T = \Sigma, ~~ \frac{1}{n}\sum_{i=1}^n x_i = \mu$$

- Find the relation between set of points and ($\mu$, $\Sigma$) and $(\mu, \Sigma)$ is known
\textcolor{\solColor}{\textit{Solution:}
$WLOG$(Without Loss of Generality), let $\mu = 0$. Since $\Sigma$ is a symmetric positive semi-definite matrix, we can perform eigen decomposition as follows:
$$
\Sigma = UDU^T = \sum_{i=1}^m (d_iu_iu_i^T).$$
where $U$ and $U^T$ are orthogonal matrix.
Let $n=2m$ and construct a set of points $x_1, ..., x_m, ..., x_{2m}$
where $x_i = \sqrt{d_i}u_i \quad$ and $\quad x_{m+i} = -\sqrt{d_i}u_i \ \forall\  1\leq i \leq m.$ Then, 
$$
\begin{aligned}
\frac{1}{n}\sum_{i=1}^{n}x_i &= \mu = 0 \\ 
\frac{1}{n}\sum_{i=1}^{n}x_ix_i^T &= \sum_{i=1}^{m}(d_iu_iu_i^T) =UDU^T =  \Sigma
\end{aligned}
$$
Note that lots of students just use the covariance of eigen decomposition to construct $\{x_i\}_{i=1}^n$. However, It should satisfy the condition that $\frac{1}{n}\sum_{i=1}^n x_i = \mu$
}

\item Let $1 \leq k \leq m$, solve the following optimization problem (and justify with proof): \\
minimize  $\quad Trace(\Phi^T \Sigma \Phi)$ \\ 
subject to $\quad \Phi^T \Phi = I_k$ \\
variables $\quad \Phi \in R^{m \times k}$
\end{enumerate}

\textcolor{\solColor}{\textit{Solution:}
Let $\phi_1, \cdots, \phi_k$ be the columns of $\Phi$. Then
$$
Trace(\Phi^T\Sigma\Phi) = \sum_{i=1}^k \phi_i^T\Sigma \phi_i = \sum_{i=1}^k \phi_i^T(\sum_{j=1}^m (d_ju_ju_j^T) )\phi_i = \sum_{j=1}^m d_j\sum_{i=1}^k\langle u_j, \phi_i\rangle^2 = \sum_{j=1}^m c_jd_j
$$
where $\langle\cdot, \cdot\rangle$ is standard inner product in Euclidean space, $c_j:= \sum_{i=1}^k\langle u_j, \phi_i\rangle^2$ for each $j=1, \cdots, m$ and $d_1\geq d_2\geq \cdots\geq d_m\geq 0$
Claim: $0\leq c_j\leq 1$ and $\sum_{j=1}^m c_j = k$
Clearly, $c_j\geq 0$. Extending $\phi_1, \cdots, \phi_k$ to $\phi_1, \cdots, \phi_k, \phi_{k+1}, \cdots, \phi_m$ for $\mathbb{R}^m$. Then, for each $j=1, \cdots, m$
$$
c_j = \sum_{i=1}^k \langle u_j, \phi_i\rangle^2 \leq \sum_{i=1}^m \langle u_j, \phi_i\rangle^2 = 1
$$
Finally, since $u_1, \cdots, u_m$ is an orthonormal basis for $\mathbb{R}^m$,
\begin{align*}
    \sum_{j=1}^{m} c_{j}=\sum_{j=1}^{m} \sum_{i=1}^{k}\left\langle{u}_{j}, {\phi}_{i}\right\rangle^{2}=\sum_{i=1}^{k} \sum_{j=1}^{m}\left\langle{u}_{j}, {\phi}_{i}\right\rangle^{2}=\sum_{i=1}^{k}\left\|{\phi}_{i}\right\|_{2}^{2}=k
\end{align*}
Hence, the minimum value of $\sum_{j=1}^m c_jd_j$ over all choice of $c_1, c_2, \cdots, c_m\in[0,1]$ with $\sum_{j=1}^k c_j=k$ is $d_{m-k+1}, \cdots, d_{m}$. This is achieved when $c_1, \cdots, c_{m-k} = 0$ and $c_{m-k+1} = \cdots = c_m = 1$
}
\begin{thebibliography}{50}
	\bibitem{Ioffe2015} Sergey Ioffe and Christian Szegedy (2015), ``Batch Normalization: Accelerating Deep Network Training b
y Reducing Internal Covariate Shift", Arxiv:1502.03167
\end{thebibliography}
\newpage
\section*{Problem 4 (Convergence of K-means Clustering)
(1.5\%)}
 In the K-means clustering algorithm, we are given a set of $n$ points $x_i \in \mathbb{R}^d, i \in\{1, \ldots, n\}$ and we want to find the centers of $k$ clusters $\boldsymbol{\mu}=\left(\mu_1, \ldots, \mu_k\right)$ by minimizing the average distance from the points to the closest cluster center. In general, $n\geq k$. Define function $\mathcal{C} :\{1, \cdots, n\}\rightarrow \{1, 2,\cdots, k\}$ assigns one of $k$ clusters to each point in the data set such that $\mathcal{C}(i) = q$ if the $i$-th data point $x_i$ is assigned to the $q$-th cluster where $i\in\{1, 2, \cdots, n\}$ and $q\in\{1, 2, \cdots, k\}$

Formally, we want to minimize the following loss function

$$
L(\mathcal{C}, \boldsymbol{\mu})=\sum_{i=1}^n\left\|x_i-\mu_{\mathcal{C}(i)}\right\|_2^2 = \sum_{q=1}^{k}\sum_{i:\mathcal{C}(i)=q}\left\|x_i-\mu_{q}\right\|_2^2
$$

The K-means algorithm:
\begin{algorithm}
\caption{K-means algorithm}

Initialize cluster center $\mu_j, j = 1, 2, \cdots, k$ ($k$ random $x_n$ from data set)

Repeat:
\begin{enumerate}
    \item Fix $\boldsymbol{\mu}$, update $\mathcal{C}(i)$ for each $i$ that minimizes $L$. Formally, consider a data point $x_i$, and let $\mathcal{C}(i)$ be the  assignment from the previous iteration and $C^*(i)$ be the new assignment obtained as: $C^*(i)= \arg\min_{j=1, \cdots, k} \Vert x_i - \mu_j\Vert^2_2$

    \item Fix $\mathcal{C}$, update the centers $\mu_j$ which satisfies 
$$\left|\left\{i: \mathcal{C}(i)=j\right\}\right|\mu_j= 
\sum_{i: \mathcal{C}(i)=j} x_i, 
$$for each $j$, where $\left|\left\{i: \mathcal{C}(i)=j\right\}\right|$ is the number of elements of set $\left\{i: \mathcal{C}(i)=j\right\}$.(i.e. Set the cluster centres to be the means of the points in each cluster.)

\end{enumerate}
\end{algorithm}

The algorithm stops when no change in loss function occurs during the assignment step.

Suppose that the algorithm proceeds from iteration $t$ to $t+1$.
\begin{enumerate}
    \item Consider the points $z_1, z_2, \cdots, z_m$, where $m\geq 1$ . and for $i\in\{1, 2, \cdots, m\}, z_i\in \mathbb{R}^d$. Let $\bar{z} = \frac{1}{m}\sum_{i=1}^m z_i$ be the mean of these points, and let $z\in \mathbb{R}^d$ be an arbitrary point in the same ($d$-dimensional) space. Then
    $$
    \sum_{i=1}^m \Vert z_i - z\Vert^2_2 \geq \sum_{i=1}^m \Vert z_i - \bar{z}\Vert^2_2
    $$
    \textcolor{\solColor}{\textit{Solution:}
    $$
\begin{aligned}
\sum_{i=1}^m\left\|\boldsymbol{z}^i-\boldsymbol{z}\right\|^2 &=\sum_{i=1}^m\left\|\left(\boldsymbol{z}^i-\overline{\boldsymbol{z}}\right)+(\overline{\boldsymbol{z}}-\boldsymbol{z})\right\|^2 \\
&=\sum_{i=1}^m\left(\left\|\boldsymbol{z}^i-\overline{\boldsymbol{z}}\right\|^2+\|\overline{\boldsymbol{z}}-\boldsymbol{z}\|^2+2\left(\boldsymbol{z}^i-\overline{\boldsymbol{z}}\right) \cdot(\overline{\boldsymbol{z}}-\boldsymbol{z})\right) \\
&=\sum_{i=1}^m\left\|\boldsymbol{z}^i-\overline{\boldsymbol{z}}\right\|^2+\sum_{i=1}^m\|\overline{\boldsymbol{z}}-\boldsymbol{z}\|^2+2 \sum_{i=1}^m\left(\boldsymbol{z}^i \cdot \overline{\boldsymbol{z}}-\boldsymbol{z}^i \cdot \boldsymbol{z}-\overline{\boldsymbol{z}} \cdot \overline{\boldsymbol{z}}+\overline{\boldsymbol{z}} \cdot \boldsymbol{z}\right) \\
&=\sum_{i=1}^m\left\|\boldsymbol{z}^i-\overline{\boldsymbol{z}}\right\|^2+m\|\overline{\boldsymbol{z}}-\boldsymbol{z}\|^2+2(m \overline{\boldsymbol{z}} \cdot \overline{\boldsymbol{z}}-m \overline{\boldsymbol{z}} \cdot \boldsymbol{z}-m \overline{\boldsymbol{z}} \cdot \overline{\boldsymbol{z}}+m \overline{\boldsymbol{z}} \cdot \boldsymbol{z}) \\
&=\sum_{i=1}^m\left\|\boldsymbol{z}^i-\overline{\boldsymbol{z}}\right\|^2+m\|\overline{\boldsymbol{z}}-\boldsymbol{z}\|^2 \\
& \geq \sum_{i=1}^m\left\|\boldsymbol{z}^i-\overline{\boldsymbol{z}}\right\|^2 .
\end{aligned}
$$
    }
    \item Show that $L(\mathcal{C}^{t+1}, \boldsymbol{\mu}^{t}) \leq L(\mathcal{C}^{t}, \boldsymbol{\mu}^{t})$ i.e. The first step in K-means clustering
    \textcolor{\solColor}{\textit{Solution:}
    It follows directly from the logic of the algorithm: $\mathcal{C}^t$ and $\mathcal{C}^{t+1}$ are different only if there is a point that finds a closer cluster center in $\boldsymbol{\mu}^t$ than the one assigned to it by $\mathcal{C}^t$:
$$
L\left(\mathcal{C}^{t+1}, \boldsymbol{\mu}^t\right)=\sum_{i=1}^n\left\|\boldsymbol{x}^i-\boldsymbol{\mu}_{C^{t+1}(i)}^t\right\|^2<\sum_{i=1}^n\left\|\boldsymbol{x}^i-\boldsymbol{\mu}_{C^t(i)}^t\right\|^2=L\left(\mathcal{C}^t, \boldsymbol{\mu}^t\right)
$$
    }

    \item Show that $L(\mathcal{C}^{t+1}, \boldsymbol{\mu}^{t+1})\leq L(\mathcal{C}^{t+1}, \boldsymbol{\mu}^{t})$ i.e. The second step in K-means clustering. (Hint: Use the result of (a))
    \textcolor{\solColor}{\textit{Solution:}
    Use the result in (1):
$$
\begin{aligned}
L\left(\mathcal{C}^{t+1}, \boldsymbol{\mu}^{t+1}\right) &=\sum_{i=1}^n\left\|\boldsymbol{x}^i-\boldsymbol{\mu}_{C^{t+1}(i)}^{t+1}\right\|^2 \\
&=\sum_{k^{\prime}=1}^k \sum_{i \in\{1,2, \ldots, n\}, \mathcal{C}^{t+1}(i)=k^{\prime}}\left\|\boldsymbol{x}^i-\boldsymbol{\mu}_{C^{t+1}(i)}^{t+1}\right\|^2 \\
& \leq \sum_{k^{\prime}=1}^k \sum_{i \in\{1,2, \ldots, n\}, \mathcal{C}^{t+1}(i)=k^{\prime}}\left\|\boldsymbol{x}^i-\boldsymbol{\mu}_{C^{t+1}(i)}^t\right\|^2 (\text{by } 1)\\
&=\sum_{i=1}^n\left\|\boldsymbol{x}^i-\boldsymbol{\mu}_{C^{t+1}(i)}^t\right\|^2 \\
&=L\left(\mathcal{C}^{t+1}, \boldsymbol{\mu}^t\right) .
\end{aligned}
$$
    }
    \item Use the result in (b) and $(c)$ to show that the loss of $k$-means clustering algorithm is monotonic decreasing.(Hint: Show that the sequence $\{l_t\}$, where $l_t = L(\mathcal{C}^{t}, \boldsymbol{\mu}^{t})$, which is monotone decreasing ($l_{t+1}\leq l_t, \forall t$) and bounded below ($l_t\geq 0$). Then, we use monotone convergence theorem for sequences, $\{l_t\}$ converges.)
    \textcolor{\solColor}{\textit{Solution:}
    Define the sequence $\{l_t\}$, where $l_t = L(\mathcal{C}^t, \boldsymbol{\mu}^t)$. By previous results, we have
$$
l_t = L(\mathcal{C}^t, \boldsymbol{\mu}^t)\leq L(\mathcal{C}^{t+1}, \boldsymbol{\mu}^{t+1}) = l_{t+1}
$$for all $t$. Hence, $\{l_t\}$ is a monotonic decreasing sequence.
Note that we apply \textbf{monotonic convergence theorem of sequence} to prove the sequence is convergence, which does not guarantee this algorithm could find the \textbf{global} minimum, just a \textbf{local} minimum.
    }
    \item Show that the $k$-means clustering algorithm converges in finitely many steps.
    \textcolor{\solColor}{\textit{Solution:}
    There are at most $k^N$ ways to partition $N$ data points into $k$ clusters. Then, this algorithm converges in finitely many steps. 
Note that the upper bound ($k^N$) may not tight.
    }
\end{enumerate}

\newpage
\section*{Problem 5 (Gradient Descent Convergence) (1.5\%)}
 Suppose the function $f:\mathbb{R}^n \rightarrow \mathbb{R}$ is differentiable. Also, $f$ is $\beta$-smoothness and $\alpha$-strongly convex. \\
 $$\beta-smoothness: \beta > 0, \forall \boldsymbol{x}, \boldsymbol{y} \in \mathbb{R}^n, 
 \lVert \nabla f(\boldsymbol{x})- \nabla f(\boldsymbol{y}) \rVert_2  \leq  \beta \lVert \boldsymbol{x} - \boldsymbol{y} \rVert_2$$ \\
 $$\alpha-strongly \ convex: \alpha > 0, \forall \boldsymbol{x}, \boldsymbol{y} \in \mathbb{R}^n, 
 f(\boldsymbol{x})-f(\boldsymbol{y})-\nabla f(\boldsymbol{y})^T (\boldsymbol{x} - \boldsymbol{y}) \geq \frac{\alpha}{2} \lVert \boldsymbol{x} - \boldsymbol{y} \rVert_2^2$$ \\
 Then we propose a gradient descent algorithm
 \begin{enumerate}
    \item Find a initial $\boldsymbol{\theta}^0.$
     \item Let $\boldsymbol{\theta}^{n+1} = \boldsymbol{\theta}^{n} - \eta \nabla_{\boldsymbol{\theta}}f(\boldsymbol{\theta}^n) \ \forall n \geq 0$, where $\eta=\frac{1}{\beta}.$
 \end{enumerate}
The following problems lead you to prove the gradient descent convergence.
\begin{enumerate}
\item Prove the property of $\beta$-smoothness function
 $$\forall \boldsymbol{x}, \boldsymbol{y} \in \mathbb{R}^n, 
 f(\boldsymbol{x})-f(\boldsymbol{y})-\nabla f(\boldsymbol{y})^T (\boldsymbol{x}-\boldsymbol{y}) \leq \frac{\beta}{2} \lVert \boldsymbol{x}-\boldsymbol{y} \rVert_2^2$$
 \begin{enumerate}
 \item Define $g: \mathbb{R} \rightarrow \mathbb{R}, g(t)=f(\boldsymbol{y}+t(\boldsymbol{x}-\boldsymbol{y}))$. Show that $f(\boldsymbol{x})-f(\boldsymbol{y})=\int_{0}^{1} g^{'}(t) \,dt$.

 \textcolor{\solColor}{\textit{Solution:}
$f(\boldsymbol{x}-\boldsymbol{y}) = g(1)-g(0)=\int_0^1 g'(t)dt$.
}
 \item Show that $g^{'}(t)= \nabla f(\boldsymbol{y}+t(\boldsymbol{x}-\boldsymbol{y}))^T(\boldsymbol{x}-\boldsymbol{y})$.
 
 \textcolor{\solColor}{\textit{Solution:}
 Let $\boldsymbol{z}=\boldsymbol{y}+t(\boldsymbol{x}-\boldsymbol{y})$. \\
$g'(t)= \frac{d g(t)}{dt}=\frac{d f(\boldsymbol{z})}{dt}= \sum_{i=1}^n \frac{d f(\boldsymbol{z})}{\boldsymbol{z}_i}\frac{d \boldsymbol{z}_i}{dt} = \sum_{i=1}^n \frac{d f(\boldsymbol{z})}{\boldsymbol{z}_i}\frac{d (\boldsymbol{y_i}+t(\boldsymbol{x_i}-\boldsymbol{y_i}))}{dt}=\sum_{i=1}^n \frac{d f(\boldsymbol{z})}{\boldsymbol{z}_i}(\boldsymbol{x}_i-\boldsymbol{y}_i)=\nabla f(\boldsymbol{z})^T(\boldsymbol{x}-\boldsymbol{y})=\nabla f(\boldsymbol{y}+t(\boldsymbol{x}-\boldsymbol{y}))^T(\boldsymbol{x}-\boldsymbol{y})$
}


 \item Show that $|f(\boldsymbol{x})-f(\boldsymbol{y})- \nabla f(\boldsymbol{y})^T(\boldsymbol{x}-\boldsymbol{y})| \leq \int_{0}^{1} |(\nabla f(\boldsymbol{y}+t(\boldsymbol{x}-\boldsymbol{y})) - \nabla f(\boldsymbol{y}))^T(\boldsymbol{x}-\boldsymbol{y})| \, dt$.
 
  \textcolor{\solColor}{\textit{Solution:}
$$|f(\boldsymbol{x})-f(\boldsymbol{y})- \nabla f(\boldsymbol{y})^T(\boldsymbol{x}-\boldsymbol{y})| = | \int_0^1 g'(t)dt - \nabla f(\boldsymbol{y})^T(\boldsymbol{x}-\boldsymbol{y}) | $$
$$=  | \int_0^1\nabla f(\boldsymbol{y}+t(\boldsymbol{x}-\boldsymbol{y}))^T(\boldsymbol{x}-\boldsymbol{y}) dt - \int_0^1 \nabla f(\boldsymbol{y})^T(\boldsymbol{x}-\boldsymbol{y}) dt|$$
$$=| \int_0^1 (\nabla f(\boldsymbol{y}+t(\boldsymbol{x}-\boldsymbol{y}))^T - \nabla f(\boldsymbol{y})^T)(\boldsymbol{x}-\boldsymbol{y}) dt |$$
}
 \item By Cauchy-Schwarz inequality and the definition of $\beta$-smoothness, show that $|f(\boldsymbol{x})-f(\boldsymbol{y})- \nabla f(\boldsymbol{y})^T(\boldsymbol{x}-\boldsymbol{y})| \leq \frac{\beta}{2} \lVert \boldsymbol{x}-\boldsymbol{y} \rVert_2^2$, hence we get $$f(\boldsymbol{x})-f(\boldsymbol{y})-\nabla f(\boldsymbol{y})^T (\boldsymbol{x}-\boldsymbol{y}) \leq \frac{\beta}{2} \lVert \boldsymbol{x}-\boldsymbol{y} \rVert_2^2$$

   \textcolor{\solColor}{\textit{Solution:}
By Cauchy-Schwarz inequality, $$| \int_0^1 (\nabla f(\boldsymbol{y}+t(\boldsymbol{x}-\boldsymbol{y}))^T - \nabla f(\boldsymbol{y})^T)(\boldsymbol{x}-\boldsymbol{y}) dt |$$
$$ \leq \int_0^1 | (\nabla f(\boldsymbol{y}+t(\boldsymbol{x}-\boldsymbol{y}))^T - \nabla f(\boldsymbol{y})^T)(\boldsymbol{x}-\boldsymbol{y}) |dt $$
$$ \leq \int_0^1 | (\nabla f(\boldsymbol{y}+t(\boldsymbol{x}-\boldsymbol{y})) - \nabla f(\boldsymbol{y}))^T(\boldsymbol{x}-\boldsymbol{y}) |dt $$
$$ \leq \int_0^1 ||(\nabla f(\boldsymbol{y}+t(\boldsymbol{x}-\boldsymbol{y})) - \nabla f(\boldsymbol{y}))||_2 ||(\boldsymbol{x}-\boldsymbol{y})||_2 dt \quad \text{By Cauchy-Schwarz inequality} $$
$$ \leq \int_0^1 \beta ||t(\boldsymbol{x}-\boldsymbol{y})||_2(\boldsymbol{x}-\boldsymbol{y})||_2 dt \leq \int_0^1 t \beta ||(\boldsymbol{x}-\boldsymbol{y})||_2^2 dt=\beta ||(\boldsymbol{x}-\boldsymbol{y})||_2^2\int_0^1 tdt = \frac{\beta}{2} ||(\boldsymbol{x}-\boldsymbol{y})||_2^2$$
}
 \end{enumerate}
 \item Let $\boldsymbol{y} = \boldsymbol{x} - \frac{1}{\beta} \nabla f(\boldsymbol{x})$ and use 1., prove that 
 $$f(\boldsymbol{x} - \frac{1}{\beta} \nabla f(\boldsymbol{x})) - f(\boldsymbol{x}) \leq -\frac{1}{2\beta} \lVert \nabla f(\boldsymbol{x}) \rVert_2^2$$
 and 
 $$f(\boldsymbol{x}^{*}) - f(\boldsymbol{x}) \leq -\frac{1}{2\beta} \lVert \nabla f(\boldsymbol{x}) \rVert_2^2,$$
 where $\boldsymbol{x}^{*} = \mathop{\arg\min}\limits_{\boldsymbol{x}} f(\boldsymbol{x})$.

   \textcolor{\solColor}{\textit{Solution:} 
By the fact that $f(\boldsymbol{y})-f(\boldsymbol{x})-\nabla f(\boldsymbol{x})^T (\boldsymbol{y}-\boldsymbol{x}) \leq \frac{\beta}{2} \lVert \boldsymbol{x}-\boldsymbol{y} \rVert_2^2$,
$$f(\boldsymbol{x}) - f(\boldsymbol{x} - \frac{1}{\beta} \nabla f(\boldsymbol{x})) - \nabla f(\boldsymbol{x})^T \frac{1}{\beta} \nabla f(\boldsymbol{x}) \leq \frac{1}{2\beta} \lVert \nabla f(\boldsymbol{x}) \rVert_2^2.$$ Also, $\nabla f(\boldsymbol{x})^T \frac{1}{\beta} \nabla f(\boldsymbol{x})=\frac{1}{\beta} \lVert \nabla f(\boldsymbol{x}) \rVert_2^2$. Hence, 
 $$f(\boldsymbol{x} - \frac{1}{\beta} \nabla f(\boldsymbol{x})) - f(\boldsymbol{x}) \leq -\frac{1}{2\beta} \lVert \nabla f(\boldsymbol{x}) \rVert_2^2$$
Owing to the fact that $f(\boldsymbol{x^*}) \leq f(\boldsymbol{x}) - \frac{1}{\beta}$, we get 
 $$f(\boldsymbol{x}^{*}) - f(\boldsymbol{x}) \leq -\frac{1}{2\beta} \lVert \nabla f(\boldsymbol{x}) \rVert_2^2.$$
}
 
 \item Show that $\forall n \geq 0$, 
 $$\lVert \boldsymbol{\theta}^{n+1} - \boldsymbol{\theta}^{*} \rVert_2^2= \lVert \boldsymbol{\theta}^{n} - \boldsymbol{\theta}^{*} \rVert_2^2 + \eta^2\lVert \nabla_{\boldsymbol{\theta}} f(\boldsymbol{\theta}^n) \rVert_2^2 -2 \eta \nabla_{\boldsymbol{\theta}} f(\boldsymbol{\theta}^n)^T(\boldsymbol{\theta}^n-\boldsymbol{\theta}^{*}),$$ where $\boldsymbol{\theta}^{*} = \mathop{\arg\min}\limits_{\boldsymbol{\theta}} f(\boldsymbol{\theta})$.\\

   \textcolor{\solColor}{\textit{Solution:} 
$$\lVert \boldsymbol{\theta}^{n+1} - \boldsymbol{\theta}^{*} \rVert_2^2= \lVert \boldsymbol{\theta}^{n} - \eta \nabla_{\boldsymbol{\theta}} f(\boldsymbol{\theta}^n) - \boldsymbol{\theta}^{*} \rVert_2^2$$ 
$$(\boldsymbol{\theta}^{n} - \eta \nabla_{\boldsymbol{\theta}} f(\boldsymbol{\theta}^n) - \boldsymbol{\theta}^{*})^T(\boldsymbol{\theta}^{n} - \eta \nabla_{\boldsymbol{\theta}} f(\boldsymbol{\theta}^n) - \boldsymbol{\theta}^{*})=\lVert \boldsymbol{\theta}^{n} - \boldsymbol{\theta}^{*} \rVert_2^2 + \eta^2\lVert \nabla_{\boldsymbol{\theta}} f(\boldsymbol{\theta}^n) \rVert_2^2 -2 \eta \nabla_{\boldsymbol{\theta}} f(\boldsymbol{\theta}^n)^T(\boldsymbol{\theta}^n-\boldsymbol{\theta}^{*})$$ 
}
 
 \item Use 2. and the definition of $\alpha$-strongly convex to prove $\forall n \geq 0$
 $$ \lVert \boldsymbol{\theta}^{n+1} - \boldsymbol{\theta}^{*} \rVert_2^2 \leq (1- \frac{\alpha}{\beta}) \lVert \boldsymbol{\theta}^{n} - \boldsymbol{\theta}^{*} \rVert_2^2, $$ where $\boldsymbol{\theta}^{*} = \mathop{\arg\min}\limits_{\boldsymbol{\theta}} f(\boldsymbol{\theta})$.\\

    \textcolor{\solColor}{\textit{Solution:} 
By rearranging the inequality we get in 2., $ \eta^2 \lVert \nabla f(\boldsymbol{\theta}^n) \rVert_2^2 \leq 2 \beta \eta^2 (f(\boldsymbol{\theta}^*) - f(\boldsymbol{\theta}^n))$.\\
Also, by the definition of $\alpha$-strongly convex, $-2 \eta \nabla_{\boldsymbol{\theta}} f(\boldsymbol{\theta}^n)^T(\boldsymbol{\theta}^n-\boldsymbol{\theta}^{*}) \leq -\alpha \eta ||\boldsymbol{\theta}^{n} - \boldsymbol{\theta}^{*}||_2^2 - 2\eta(f(\boldsymbol{\theta}^*)-f(\boldsymbol{\theta}^n))$
Combined the above results and $\eta = \frac{1}{\beta}$, we get 
$$\lVert \boldsymbol{\theta}^{n+1} - \boldsymbol{\theta}^{*} \rVert_2^2 \leq \lVert \boldsymbol{\theta}^{n} - \boldsymbol{\theta}^{*} \rVert_2^2 + \frac{2}{\beta} (f(\boldsymbol{\theta}^*) - f(\boldsymbol{\theta}^n)) -\frac{\alpha}{\beta} ||\boldsymbol{\theta}^{n} - \boldsymbol{\theta}^{*}||_2^2 - \frac{2}{\beta}(f(\boldsymbol{\theta}^*)-f(\boldsymbol{\theta}^n)) = (1- \frac{\alpha}{\beta}) \lVert \boldsymbol{\theta}^{n} - \boldsymbol{\theta}^{*} \rVert_2^2.$$
}


 \item Use the above inequality to show that $\boldsymbol{\theta}^n$ will converge to $\boldsymbol{\theta}^*$ when $n$ goes to infinity.

    \textcolor{\solColor}{\textit{Solution:} 
Because $\frac{\alpha}{\beta}>0$, also, by the property of $\beta$-smoothness function and the definition of $\alpha$ strongly convex function, we get $\alpha \leq \beta$, which derives $\frac{\alpha}{\beta}<1$ and $0<1-\frac{\alpha}{\beta}<1$. When $n$ goes to infinity, $||\boldsymbol{\theta^n}-\boldsymbol{\theta^*}||=0$. Hence, $\boldsymbol{\theta^n}$ will converge to $\boldsymbol{\theta^*}$
}
 \end{enumerate}

 \section*{Version Description}
 \begin{enumerate}
     \item First Edition: Finish Problem Solution 1 to 5
 \end{enumerate}
\end{document}
